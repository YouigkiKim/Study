{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"at a Saturday matinee in my home town. I went with an older friend (he was about 12) and my mom let me go because she thought the film would be OK (it\\'s rated G). I was assaulted by loud music, STRANGE images, no plot and a stubborn refusal to make ANY sense. We left halfway through because we were bored, frustrated and our ears hurt. <br /><br />I saw it 22 years later in a revival theatre. My opinion had changed--it\\'s even WORSE! Basically everything I hated about it was still there and the film was VERY 60s...and has dated badly. I got all the little in-jokes...too bad they weren\\'t funny. The constant shifts in tone got quickly annoying and there\\'s absolutely nothing to get a firm grip on. Some people will love this. I found it frustrating...by the end of the film I felt like throwing something heavy at the screen.<br /><br />Also, all the Monkees songs in this movie SUCK (and I DO like them).<br /><br />For ex-hippies only...or if you\\'re stoned. I give this a 1.\"',\n",
       " 0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer(text): \n",
    "    # HTML 태그 제거\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    # 이모티콘 패턴 검색 (\":\", \";\", \"=\" 기호로 시작하고, 그 뒤에 \")\", \"(\", \"D\", \"P\"가 올 수 있음)\n",
    "    emoticons = re.findall(r'[:;=](?:-)?[)(DP]', text)\n",
    "    \n",
    "    # 텍스트에서 모든 특수문자와 숫자를 공백으로 대체하고, 이모티콘은 다시 추가\n",
    "    # text = re.sub(r'[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "            + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding = 'utf-8') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int (line[-2])\n",
    "            yield text, label\n",
    "#next는 머리행을 넘어가기 위해\n",
    "next(stream_docs(path='data/movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [],[]\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features = 2**21,\n",
    "                         preprocessor= None,\n",
    "                         tokenizer = tokenizer)\n",
    "clf = SGDClassifier(loss = 'log_loss', random_state = 1, max_iter = 1)\n",
    "doc_stream = stream_docs(path = 'data/movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import sys\n",
    "pbar = pyprind.ProgBar(45, stream = sys.stdout)\n",
    "classes = np.array([0,1])\n",
    "\n",
    "for _ in range(45):\n",
    "    X_train , y_train = get_minibatch(doc_stream , size = 1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes = classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.873\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size = 5000)\n",
    "X_vec = vect.transform(X_test)\n",
    "print(f\"정확도 : {clf.score(X_vec, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8962\n"
     ]
    }
   ],
   "source": [
    "clf = clf.partial_fit(X_vec, y_test)\n",
    "print(clf.score(X_vec, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
